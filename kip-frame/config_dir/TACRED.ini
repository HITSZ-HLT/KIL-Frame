[training]
batch_size = 64
replay_bs = 64
gradient_accumulation_steps = 1
total_round = 5
rel_per_task = 4
drop_out = 0.5
num_workers = 4
step1_epochs = 10
step2_epochs = 3
step3_epochs = 10
num_protos = 10
device = cuda:5
seed = 2021
max_grad_norm = 10
mem_lr = 1e-4
encoder_lr = 1e-5
score_weight = 20.0

[Encoder]
bert_path = ./bert_chk
max_length = 256
vocab_size = 30522
encoder_output_size = 768

[memory]
key_size = 256
head_size = 768
mem_size = 768

[data]
data_file = ./data/tacred/data_with_marker_tacred.json
relation_file = ./data/tacred/id2rel_tacred.json
num_of_relation = 40
num_of_train = 420
num_of_val = 140
num_of_test = 140
task_name = 'TACRED'
init_new_token_by_cls = False
proto_patern = cls
template_id = 2
mem_alpha = 0.1
sampling_method = kmeans

[extra]
use_strictloss = False
few_shot = 10000
p_mask = 0
cluster_split_data = False
proto_with_ln = False
proto_wo_prompt = True
avg_ptototype = False
num_head = 12
use_marker = False
mlm_loss_weight = 0.0
balance_loss_weight = 5.0
rewrite_p = 0.0